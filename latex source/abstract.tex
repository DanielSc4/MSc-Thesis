% ************************** Thesis Abstract *****************************
% Use `abstract' as an option in the document class to print only the titlepage and the abstract.
\begin{abstract}

\begin{center}
    {\color[HTML]{F8A102} \faIcon{exclamation-triangle} This thesis contains examples which are offensive in nature.}    
\end{center}


\paragraph{English} Due to language models' propensity to generate toxic or hateful responses, several techniques were developed to align model generations with users' preferences. Despite the effectiveness of such methods in improving the safety of model interactions, their impact on models' internal processes is still poorly understood. 
In this work, we apply popular detoxification approaches to several language models, find a trade-off between helpfulness and harmlessness of the models, and finally quantify their impact on the resulting models' prompt dependence using feature attribution methods. We evaluate the effectiveness of counter-narrative fine-tuning and compare it with reinforcement learning-driven detoxification, observing differences in prompt reliance between the two methods despite their similar detoxification performances. The results lead to new hypotheses regarding the interpretability of detoxification processes that could be exploited to improve the state of the art in this domain.

\begin{center}
    {\color[HTML]{F8A102} \faIcon{exclamation-triangle} Questa tesi contiene esempi di natura offensivi}
\end{center}


\paragraph{Italiano} A causa della propensione dei modelli linguistici a generare risposte tossiche o discorsi d'odio, sono state sviluppate diverse tecniche per allineare le generazioni dei modelli alle preferenze degli umane. Nonostante l'efficacia di questi metodi nel migliorare la sicurezza nell'interazione con i modelli, il loro impatto sui meccanismi interni dei modelli è ancora poco esplorato. 
In questo lavoro, applichiamo i più diffusi approcci di disintossicazione a diversi language models, troviamo un compromesso tra la capacità di aiutare e la sicurezza in termini di inoffensività dei modelli e, infine, quantifichiamo la dipendenza da prompt dei modelli risultanti utilizzando metodi di attribuzione. Valutiamo l'efficacia del fine-tuning con contro-narrativa e lo confrontiamo con la disintossicazione guidata dall'apprendimento per rinforzo, osservando differenze nella dipendenza da prompt tra i due metodi nonostante le prestazioni di disintossicazione simili tra loro. I risultati portano a nuove ipotesi sull'interpretabilità dei processi di disintossicazione, che potrebbero essere sfruttate per migliorare lo stato dell'arte in questo campo.

\end{abstract}
