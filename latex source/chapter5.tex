
\chapter{Discussion \& Broader Impact}

The thesis work brought forth interesting insights into the use of interpretability techniques in model detoxification. 

As introduced, Large Language Models, despite their rise in capability in recent years, carry with them a number of issues that, industry and research aim to mitigate. These problems, of all things, are especially evident in the presence of large models that can express what they have previously seen during their pre-training process. It was explained how, given the enormous amount of data collected and used to pre-train the models, they inevitably incorporate toxicity features, different kinds of bias against more debile categories, and all the features that can be found in an uncontrolled place like the Web. In addition to data curation, efforts then turned to try to make these models less toxic by adopting various techniques that would allow them to change behaviour during generation.

Looking at the state of the art, the two main techniques used today for precisely these processes were analyzed during the paper: fine-tuning with instruction datasets and reinforcement learning to align model output with human preferences. Both of these processes, however, are very often adopted without looking at the intrinsic workings and especially their effect on the models. Therefore, the tendency to measure the performance of models, after their detoxification process, by following metrics that try to express toxicity on standard datasets is criticized. 

For this reason, in this thesis work, initial steps are taken toward explaining these techniques, with the goal of making the results of these processes more interpretable and thus generalizable. An attempt is made to have an explanation of the behaviour of the LMs defined as black-boxes, being able to capture the differences in capacity in the responses not only by looking at the output but by interpreting the process that the model went through to produce it. 

The coveted balance between the helpfulness and harmlessness characteristics of models is further discussed in this regard. Very often, in fact, models are detoxified toward directions that are considered safer than they should be. Thus, the goal has been to obtain a final LM that turns out to have a very low level of toxicity but at the same time is left free to respond to prompts that are considered risky or unsafe. For this reason, fine-tuning is proposed by following a counter-narrative generation, which, in the case of toxic prompts, opposes and tries to provide an explanation by helping and reasoning together with the user, instead of refusing to respond because of imposed limits. 

Based on these assumptions, the results of the proposed models were proposed, effectively succeeding in decreasing their toxicity with respect to state-of-the-art datasets. This demonstrated not only the already known effectiveness of detoxification methods but also the possibility of using counter-narratives by obtaining better models from the point of view of balancing aspects of helpfulness and harmlessness.

Having detoxified the models, according to the latest detoxification techniques based on fine-tuning with instruction on counter-narrative and reinforcement learning, we wanted to investigate what the changes were at the level of model generation. For this very reason, interpretability techniques were adopted based on the attribution scores that the model gives at the generation stage during the performance of the next token prediction task. Special emphasis was placed on attributions toward the prompt, defining prompt dependence as a measure for the model in terms of its reliance on the prompt in the token generation phase. Analyzing this proposed new metric observed trends throughout generation, shifts between different levels of generation toxicity, but most importantly a measure of how uncertain the model was toward the prompt during generation. In fact, by taking the attributions it was possible to calculate their entropy, assimilating it to the concept of uncertainty about the model looking at the distribution. Analyzing the various results showed that models, especially if fine-tuned with counter-narratives, during generation on average have higher entropy, i.e., their distribution on the scores is more spread throughout the prompt. This association allows us to highlight how the model probably does not generate based on keywords within the prompt but is focused on it by looking at it as a whole. In other words, the observed behaviour is probably related to counter-narrative generation, where the model tries to contrast the whole prompt instead of taking inspiration from individual keywords.

The work therefore concludes with a new line of research, hypothesizing to effectively explain techniques that to date have proven effective in their proposed purposes but still remain unexplained and therefore difficult to generalize in terms of accountability and model safety.


\section {Limitations and future work}
The project considers several aspects of detoxification processes currently considered to be the state of the art in the field. An initial consideration of the breadth and scalability of the observations made can be made regarding the size of the models. In fact, in the presence of many more parameters, models tend to show greater linguistic and reasoning capabilities that could accentuate or altogether change the patterns observed in the experimental phase. 

Undoubtedly, counter-narrative would allow a greater advantage in terms of balancing helpfulness and harmlessness of patterns, leading to better models to be applied in real-world contexts. Regarding the interpretability of the output of the LMs, it is certainly interesting to investigate the still unclear aspects regarding the differences between the two models (RedPajama and Falcon) used in the experiments. In this sense, it would be important to analyze the reasons for these differences and their potential generalization to a wider range of LMs. Further analyzing entropy, highlighted as being higher for the fine-tuned models with counter-narrative, it would be interesting to understand how the model rests its attention on the prompt, which terms most influence this behaviour, and how in general the distribution of attributions is related to the terminology used in the original prompt. As anticipated, results along these lines would lead to a better understanding of the model's internal phenomena, allowing generalizations about their behaviour and thus offering greater confidence regarding their stability in more critical and risky situations.


% \section {Conclusion and future work}
% \todo{none}