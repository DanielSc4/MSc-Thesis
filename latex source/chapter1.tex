% \textit{} corsivo
% \textbf{} grassetto
% \\  per andare a capo



\chapter{Introduction}
\hspace{0,5cm}

Language models (LMs) are increasingly becoming a mass-market tool, moving out of the scientific literature and beginning to find applications for use in a wide variety of work and non-work settings. Their evolution, observed especially in recent years, concerns models of language understanding based on the transformers' architecture that, for the first time, succeed in the representation of natural language, i.e., one of the least structured forms for representing information. Their impact on society makes it possible to help and automate a very large amount of tasks, which no longer includes only the solving of mechanical and deterministic tasks, but expands to topics that can be interpreted or where reasoning is required that until recently was subject exclusively to human abilities and intelligence.

Just as happened in the past with major innovations in science, the rise in capability of these models could lead to a revolution in today's society, introducing new opportunities for the use of these new technologies.

This introduction will address the reasons behind the rapid development of language models and the resulting problems that are currently recognized by the scientific community. The techniques currently used to handle these problems will then be summarily introduced and how, through the work done for this thesis, the state of the art in this field can be improved.


\section{Generative and non Language Models}

A large language model (LLM) refers to a type of artificial intelligence model designed to understand and/or generate human-like text based on the patterns and information it has learned from vast amounts of textual data. For several years now, attention-based models have represented the state of the art in natural language understanding, achieving, thanks to their scalability, performance similar to humans, both in terms of text generation and text representation capabilities. Through the observation of texts, each model is trained to understand the patterns present in natural language, trying to best represent both the syntactic and semantic characteristics of texts.

Learning is thus quite similar to what is generally attributed to human behaviour during the early stages of life. It is for this reason that, for some years now, LMs have been able to manage and represent natural language in a structured form of data, therefore understandable by machines that can perform logical operations on it. Specifically, the text representation during the early stages had very good capabilities that had never been observed but were not comparable with human capabilities in terms of comprehension and reasoning. Despite the excellent capabilities, the longer and more complex tasks, on which reasoning is required, could not be solved automatically by the models, maintaining an error threshold that was always more or less high.

However, the vast amount of data on the Web and especially the advancement of technology in engineering terms regarding computational power has led to a growth in the performance of these models, gradually catching up with the performance of humans and sometimes, in increasing cases, even exceeding the human average. These innovations, especially in recent years, if not in recent months, have brought with them a very high level of innovation, solving problems that until recently were considered very difficult if not impossible to solve logically. All this innovation in the field has led to the creation of models with billions of parameters (one of the methods of comparing the size of models) and with them, their capabilities in solving progressively more complex problems.

However, this race toward the largest and most capable model has inevitably brought with it different kinds of problems, more or less related to the architecture being used or very often related to the data being used to train the models themselves. As mentioned above, textual data, predominantly collected from the Internet, brings with it a number of issues entirely inherited from the problems present in today's society. Given the widespread use of the Internet by all of society, language models reflect all observed patterns, share the same biases observed in humans, the same beliefs and behaviours that are not always correct and justifiable.


\section{Issues of Language Models}

As anticipated, especially for LMs that are beginning to be used by the majority of the population, their issues are becoming increasingly relevant in the scientific-technical landscape.

In exclusively pre-trained models, i.e., trained to predict the most likely token (or word) that follows a string of texts, it is possible to notice certain behaviours that lead the models themselves to generate sentences that in many cases can be considered offensive, dangerous or otherwise maintain stereotypes and various more or less hidden biases toward minorities. Unfortunately, despite efforts to do so, having absolute certainty about the cleanliness and correctness of training data turns out to be a very difficult problem to solve mainly because of two factors. The first is obviously the amount of data involved and the resulting management capacity. Typically, for the largest models, data is needed in quantities on the order of hundreds of terabytes, which are practically unmanageable by any human being. The second cause stems mainly from defining when a model is safe or unsafe. Hiding specific data because it is considered dangerous or unfair turns out to be a decidedly difficult problem to solve, both technically and conceptually. Automatic identification of these issues is not always perfect, still leading to the presence of false negatives and thus not reaching surely perfect data. On the other hand, aligning the data with some guidelines would be a difficult problem to characterize \textit{a priori}, as there are no precise standards on what is permissible or not to use for training these models.

Over time, both in research and in industry, there has been a gradual shift to new techniques for handling these kinds of problems. Given the need to employ the least toxic and safest possible generative LMs among the most widely used techniques, also addressed in this thesis, are Instruction Tuning and Reinforcement Learning from Human Feedback. In both cases, these techniques are employed on the already pre-trained models to modify their behaviour under certain circumstances to allow them to generate content that would not offend or cause harm to anyone. These techniques have allowed the models' capabilities to be refined over time, even eliciting capabilities not expected as a result of the pre-training phase. The use of these techniques finally made it possible to obtain models that were much less toxic and much safer than their solely pre-trained counterparts. On the other hand, however, the use of these techniques does not fully resolve the issues previously addressed, still leaving open cases where models are led to unanticipated behaviour that may possibly offend or even cause harm to the users who use them. For this very reason, it is still necessary to work on the effectiveness of these techniques, continuing to make LMs increasingly safe tools that can be adopted by most people with as few constraints as possible. It is also necessary to ensure, as far as possible, that these techniques are truly effective in the process of detoxifying and securing the models themselves. 

The topic of model reliability and accountability appears to be still in its infancy, but it is of paramount importance to try to ensure the effectiveness of a given process on a model. Following the processes listed above, it is difficult to obtain with good certainty what changes have been made and especially how they impacted the LMs under analysis. Generally, this process relies on standard metrics on datasets recognized in the literature. These datasets attempt to best describe a broader possible space of situations that the model can deal with so that, through the use of certain metrics, we can measure how reliable and safe the model is under certain circumstances. However, the use of these datasets only ultimately tests how the model performs, inevitably leaving out aspects that it might encounter in the real world. Moreover, it is very difficult to verify the true learning of the models themselves, referred to as "stochastic parrots", repeating what was seen in the training phase and therefore not ensuring their effectiveness in a real-world context.

Help in this regard could be provided by interpreting the learning process of the model, that is, by checking in detail what processes influence the model during the production of the output. Understanding these factors would provide a more generalizable view on the learning process and therefore less tied to specific tasks or prompts that may or may not be contained in the different benchmark datasets.

\section{Language Models as black boxes}

Language models, despite their great capabilities demonstrated in a very diverse range of tasks, are still not used in particularly risky or sensitive scenarios because of their characteristic of being considered black-box or not totally reliable about their decisions. In this sense, it turns out to be really difficult to understand the reasons for a given output given by a model, generating uncertainty about the decision (or output) returned that should not be taken into account in high-risk scenarios. 

For this reason, over time, various techniques were developed to interpret the models themselves, the task of which was to understand the outputs being produced. Given the complex nature and enormity of the parameters, especially in newer language models, the development of these techniques is increasingly difficult. Attempts have been made by exploiting various model architectures, measuring the importance of initial features or even going to look at the importance of individual neurons within neural networks. Even now it is impossible to have absolute certainty about the process of producing the output, but there are several approaches proposed to measure different characteristics between models, highlighting important patterns that help in the better understanding of the same.

Therefore, in this thesis work, special emphasis is placed on the discourse of model interpretability. Several techniques will be employed whose purpose is to verify the true learning process of the models used, and further to confirm what is observed by the more classical output measurement techniques. The purpose is also to quantify the results obtained through the explanation of language models. This step would provide certainty about the training process involved, being able to measure and possibly control it. The work, specifically, will focus on a detoxification process, so that we can precisely control whether and how the training process had a concrete effect on the model under analysis.

